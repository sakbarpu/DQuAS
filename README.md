# DQuAS (Deep Question Answering System)

This is `dee-quas`, a deep learning system trained on Google Natural 
Questions Dataset. The repository contains code to fine-tune [BERT](https://arxiv.org/abs/1810.04805)
on the training dataset and for prediction on the dev dataset.

## Instructions for installing and running code 
### Prerequisites
  * Ubuntu 16.04 instance with CUDA installed
  * Python 3.7
  * Pandas 0.24.0
  * Sklearn 0.20.1
  * **Torch 1.7.0**
  * **Huggingface Transformers 2.4.0**
  * wget and gzip Linux tools for downloading and unzipping data files
  * **Hardware: Intel Core Processor with NVIDIA GTX 1080 Ti**
  
### Downloading data
  Simply use the download_dataset.py script to download Here is how to use the script:
    
    python download_dataset.py <data_dir>
  
  where `<data_dir>` is the place where you want to hold the data. Make sure there is
  enough space on disk because dataset can be many GBs.
  
  It will store the data as follows:
   
  ```
  data_dir
        ├── dev
        │   ├── nq-dev-00.jsonl
                ...
        │   └── nq-dev-04.jsonl
        └── train
            ├── nq-train-00.jsonl
                ...
            ├── nq-train-49.jsonl
  ```

### Preprocessing data
  With a single python command you can preprocess the data (both train and dev)
  
    python preprocess_data.py <data_dir>
    
  where `<data_dir>` is the place where you want to hold the data. The result will be 
  creation of two csv files train_processed.csv and dev_processed.csv.
  
  ```
  data_dir
        |_ train
                |_ train_processed.csv
        |_ dev
                |_ dev_processed.csv
  ```
  
### Train the model
 Again with a single command you can train the model.
 
   ```
   python train.py <data_dir> <model_dir>
   ```
   
 where `<data_dir>` is the path to the root of the data. And `model_dir` refers to the
 place you want to store the model checkpoints.
 
### Prediction
 This is where we run the trained model to perform predictions on the dev set and produce
 the predictions file as required by the nq_eval.py file.
 
   ```
   python prediction.py <data_dir> <model_path> <output_file>
   ```
   
 where `<data_dir>` is the root dir of the data, `<model_path>` is the path to the checkpoint
 of the model that you want to use for prediction, and `<output_file>` is the output json file
 path where you want to store the prediction results.
 
 ### Metric calculation using nq_eval.py 
 We use the [nq_eval.py](https://github.com/google-research-datasets/natural-questions/blob/master/nq_eval.py)
 script provided by the authors of the dataset to evaluate the performance of our model on
 the dev predictions.
 
 The way to call this script is straightforward:
 
 ```
   python nq_eval.py --gold_path=<path-to-gold-files> --predictions_path=<path_to_json>
 ```
 
 where `<gold_path>` is the list of raw gunzipped dev data files that were downloaded from 
 the internet and `<predictions_path>` is the output json file generated by `prediction.py` script.
  
## F1 score / Precision / Recall when running nq\_eval.py for the task

When evaluation script nq_eval.py is run against the predictions produced by the model in
prediction.py and original gold dev files, it produces the following metric values for long
answer prediction:

| Metric | Value|
---------|----------------------------------------------------
"long-best-threshold-f1" | 0.9071358748778103               
"long-best-threshold-precision" | 0.8300536672629696        
"long-best-threshold-recall" | 1.0                         
"long-best-threshold" | 0.9999969005584717                 
"long-recall-at-precision>=0.5" | 1.0                      
"long-precision-at-precision>=0.5" | 0.8300536672629696    
"long-recall-at-precision>=0.75" | 1.0                     
"long-precision-at-precision>=0.75" | 0.8300536672629696   
"long-recall-at-precision>=0.9" | 0                        
"long-precision-at-precision>=0.9" | 0                     

## Replicate these results

 Use the setting currently implemented there in the code and the results should be replicable.
 Also, we could only go up until the second checkpoint and so just used that one for prediction.
 After second checkpoint (and almost 2 days of training) we killed the training process.
 Otherwise, it would have taken more than a week to train on GTX 1080 Ti.
 
 The following are the training criteria (hyperparamters etc.) used:
 
    ```
    freeze_bert = False     # if True, freeze the encoder weights and only update the classification layer weights
    maxlen = 512  # max length of the tokenized input sentence pair : if greater than "maxlen", the input is truncated and else if smaller, the input is padded
    bs = 4  # batch size
    lr = 1e-6  # learning rate
    criterion = nn.BCEWithLogitsLoss() # this is binary cross entropy with logits loss
    opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)   
    ```

## The Neural Network Architecture 

We have used the Huggingface library to load the pretrained BERT model for fine-tuning on our
task. In the code, there is capability of playing with multiple different pretrained model.
In current state the model used is the [`bert-based-uncased`](https://huggingface.co/bert-base-uncased)
model.

The model has 110M parameters and is 

For fine-tuning on our binary classification task, we provide (question, candidate) pairs as 
input sequence and use the `[CLS]` Layer used for classification. The embedding for this layer
is sent as input to a Linear layer with just one sigmoid output. The sigmoid output tells us
whether the prediction is positive (1) or negative (0). For the training examples, the model
is trained using (question, candidate) pairs.

### Why it works?

This architecture is simple and naturally works for this binary classification problem. The problem
is naturally a binary classification problem of finding relevance/matching score between two pieces 
of texts question and candidate needs to be determined. BERT binary pair of sentence classification 
works best for this type of problem. Since, BERT is pretrained on next-sentence prediction task, it
makes it suitable for this kind of problem. 

### Sampling Heuristic

However, for each positive example (question, long answer) pair, there can be many negative examples
(question, candidate) pairs. If there are hundreds of such negative candidates then training becomes
prohibitively compute expensive. Therefore, we rely on negative sampling heuristic defined below.

We sample 10% of negative example pairs for each positive examples. So, if there are 100 negative
examples, we sample 10 of them. But, this is not just a random sampling with uniform distribution.
Rather, it's sampling based on the length of the candidate in the negative example pair.

Specifically, we sample 50% or 5 examples that have the candidate closest in length to the actual 
positive example, 25% or 3 examples that have the candidate smallest in length, and the rest 25%
or 3 examples that have the candidate largest in length. This type of sampling is performed to
ensure that we sample different lengths of candidate and also to ensure that most of the samples
have their lengths closest to the actual positive long answer.

## Improvements possible

  * We can try more recent deep learning models for training instead of BERT. 
  
  * We can also try fine-tuning BERT for longer periods of time. We have only trained it for a portion of dataset. 

  * We can use bigger BERT model.
  
  * Knowledge-aware models like RAG could be used to improve performance further. These models use knowledge-base
    as guiding force for making decisions about relevance of a text piece to a query.

  * Multi-GPU training is necessary for this kind of task for fast processing.
  
  * We can explore the dataset for types of questions asked by the user to determine the user intent.
    Based on that, we can first identify what kind of question the user is asking and then build a separate
    model for each type of intent. Also, if the intent is not decidable, we can use straightforward modeling
    approach.

## Critical analysis of the Google NQ task

In a practical scenario, there is definitely one more task that has to happen 
before this Google NQ task. And that is the retrieval of the relevant Wikipedia article.
This Google NQ task assumes we already know the article we want to find the passage in
which is not very realistic since there could be hundreds of articles that would have relevance
to the question.

Also, it shouldn't be just about Wikipedia articles, rather a multi-domain (News, Blogs, Code even)
dataset should be more complex/challenging and should be a true reflection of human-level performance
on search and retrieval tasks.

## Please donate to:

[UNICEF](https://www.unicefusa.org/mission/emergencies/child-refugees-and-migrants?form=FUNSUJMLZDZ&utm_content=taxdeduct1responsive_E2001&ms=cpc_dig_2020_Brand_20200109_google_taxdeduct1responsive_delve_E2001&initialms=cpc_dig_2020_Brand_20200109_google_taxdeduct1responsive_delve_E2001&gclid=Cj0KCQiAzsz-BRCCARIsANotFgN5fgFXSgUWaUHVRpfO37gI2DULk_Aqco9x2JrK4LNYUNhCz_cGebMaApc3EALw_wcB)
